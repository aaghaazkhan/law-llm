{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T13:46:58.687851Z","iopub.execute_input":"2025-03-22T13:46:58.688160Z","iopub.status.idle":"2025-03-22T13:46:58.692777Z","shell.execute_reply.started":"2025-03-22T13:46:58.688137Z","shell.execute_reply":"2025-03-22T13:46:58.691899Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig\n!pip install -U bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T13:46:58.693984Z","iopub.execute_input":"2025-03-22T13:46:58.694221Z","iopub.status.idle":"2025-03-22T13:47:02.196654Z","shell.execute_reply.started":"2025-03-22T13:46:58.694195Z","shell.execute_reply":"2025-03-22T13:47:02.195622Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.3)\nRequirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import json\nimport re\nimport torch\nfrom datasets import Dataset, DatasetDict\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, DataCollatorWithPadding\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import Trainer, TrainingArguments\nfrom huggingface_hub import login\n# login(token=\"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T00:11:13.099510Z","iopub.execute_input":"2025-03-23T00:11:13.099852Z","iopub.status.idle":"2025-03-23T00:11:13.104467Z","shell.execute_reply.started":"2025-03-23T00:11:13.099794Z","shell.execute_reply":"2025-03-23T00:11:13.103496Z"}},"outputs":[],"execution_count":87},{"cell_type":"code","source":"dataset = load_dataset(\"DevToAI/indian_laws_llama2_supported\")\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T13:47:02.290800Z","iopub.execute_input":"2025-03-22T13:47:02.291218Z","iopub.status.idle":"2025-03-22T13:47:03.420672Z","shell.execute_reply.started":"2025-03-22T13:47:02.291189Z","shell.execute_reply":"2025-03-22T13:47:03.419934Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 24607\n    })\n})"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"dataset['train'][0]['text']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T13:47:03.421497Z","iopub.execute_input":"2025-03-22T13:47:03.421706Z","iopub.status.idle":"2025-03-22T13:47:03.426877Z","shell.execute_reply.started":"2025-03-22T13:47:03.421689Z","shell.execute_reply":"2025-03-22T13:47:03.426096Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"'<s>[INST] Instruction:\\nWhat was the original wording of the clause in relation to the \"Legislature of the State of Mizoram\"? [/INST] Response:\\nThe original wording of the clause in relation to the \"Legislature of the State of Mizoram\" was \"fifteen years.\" </s>'"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"def convert_llama2_to_llama3_1(dataset):\n    data = {}\n\n    for split in dataset:\n        formatted_texts = []\n        \n        for example in dataset[split][\"text\"]:\n            if \"[INST]\" in example and \"[/INST]\" in example:\n                user_content = example.split(\"[INST]\")[1].split(\"[/INST]\")[0].strip()\n                assistant_content = example.split(\"[/INST]\")[1].strip().replace(\"</s>\", \"\").strip()\n\n                user_content = re.sub(r\"^Instruction:\\s*\", \"\", user_content)\n                assistant_content = re.sub(r\"^Response:\\s*\", \"\", assistant_content)\n\n                formatted_texts.append(json.dumps([\n                    {\"role\": \"user\", \"content\": user_content},\n                    {\"role\": \"assistant\", \"content\": assistant_content}\n                ], ensure_ascii=False))\n\n        data[split] = Dataset.from_dict({\"text\": formatted_texts})\n\n    return DatasetDict(data)\n\ndataset = convert_llama2_to_llama3_1(dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T13:47:03.427753Z","iopub.execute_input":"2025-03-22T13:47:03.428107Z","iopub.status.idle":"2025-03-22T13:47:04.131284Z","shell.execute_reply.started":"2025-03-22T13:47:03.428075Z","shell.execute_reply":"2025-03-22T13:47:04.130562Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T13:47:04.132162Z","iopub.execute_input":"2025-03-22T13:47:04.132424Z","iopub.status.idle":"2025-03-22T13:47:04.137235Z","shell.execute_reply.started":"2025-03-22T13:47:04.132403Z","shell.execute_reply":"2025-03-22T13:47:04.136550Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 24607\n    })\n})"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"dataset[\"train\"][3][\"text\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T13:47:04.139639Z","iopub.execute_input":"2025-03-22T13:47:04.139905Z","iopub.status.idle":"2025-03-22T13:47:04.154108Z","shell.execute_reply.started":"2025-03-22T13:47:04.139873Z","shell.execute_reply":"2025-03-22T13:47:04.153419Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'[{\"role\": \"user\", \"content\": \"How did the abolishment of the privy purse affect rulers and their successors financially?\"}, {\"role\": \"assistant\", \"content\": \"The abolishment of the privy purse affected rulers and their successors financially by ceasing to pay any sum as privy purse from the commencement of the Constitution (Twenty-sixth Amendment) Act, 1971. This effectively ended the financial support previously provided to rulers and their successors by the government.\"}]'"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"dataset = dataset['train'].train_test_split(test_size=0.1, seed=42)\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T13:47:04.155149Z","iopub.execute_input":"2025-03-22T13:47:04.155364Z","iopub.status.idle":"2025-03-22T13:47:04.180096Z","shell.execute_reply.started":"2025-03-22T13:47:04.155336Z","shell.execute_reply":"2025-03-22T13:47:04.179244Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text'],\n        num_rows: 22146\n    })\n    test: Dataset({\n        features: ['text'],\n        num_rows: 2461\n    })\n})"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"train_dataset = dataset['train']\nvalidation_dataset = dataset['test']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T13:47:04.180991Z","iopub.execute_input":"2025-03-22T13:47:04.181298Z","iopub.status.idle":"2025-03-22T13:47:04.184946Z","shell.execute_reply.started":"2025-03-22T13:47:04.181267Z","shell.execute_reply":"2025-03-22T13:47:04.183979Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"model_name = \"Qwen/Qwen2.5-3B-Instruct\" \ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\n# 4-bit quantization\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_compute_dtype = \"float16\",\n    bnb_4bit_use_double_quant = True,\n    bnb_4bit_quant_type = \"nf4\"\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config = bnb_config,\n    device_map = \"auto\",\n)\n\nlora_config = LoraConfig(\n    r = 4,\n    lora_alpha = 8,\n    lora_dropout = 0.05,\n    target_modules = [\"q_proj\", \"v_proj\"],\n    bias = \"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\nprint(\"QLoRA model loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T13:47:04.185862Z","iopub.execute_input":"2025-03-22T13:47:04.186153Z","iopub.status.idle":"2025-03-22T13:47:12.414598Z","shell.execute_reply.started":"2025-03-22T13:47:04.186124Z","shell.execute_reply":"2025-03-22T13:47:12.413736Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"274ef7fe68c04ee2b696e96c68d3bfb3"}},"metadata":{}},{"name":"stdout","text":"QLoRA model loaded successfully!\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"def tokenize_function(dataset):\n    return tokenizer(dataset[\"text\"], padding=\"max_length\", truncation=True, max_length=600)\n\ntokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\ntokenized_validation_dataset = validation_dataset.map(tokenize_function, batched=True)\n\ntokenized_train_dataset = tokenized_train_dataset.remove_columns([\"text\"])\ntokenized_validation_dataset = tokenized_validation_dataset.remove_columns([\"text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T13:47:12.415404Z","iopub.execute_input":"2025-03-22T13:47:12.415680Z","iopub.status.idle":"2025-03-22T13:47:22.279910Z","shell.execute_reply.started":"2025-03-22T13:47:12.415650Z","shell.execute_reply":"2025-03-22T13:47:22.278999Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/22146 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb4c12a92ba649e7acc49ab62b866451"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2461 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7658027c09b94ec88ef3c002bf27c9ac"}},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"def format_for_causal_lm(example):\n    example[\"labels\"] = example[\"input_ids\"].copy() \n    return example\n\ntokenized_train_dataset = tokenized_train_dataset.map(format_for_causal_lm)\ntokenized_validation_dataset = tokenized_validation_dataset.map(format_for_causal_lm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T13:47:22.280862Z","iopub.execute_input":"2025-03-22T13:47:22.281180Z","iopub.status.idle":"2025-03-22T13:47:34.217770Z","shell.execute_reply.started":"2025-03-22T13:47:22.281150Z","shell.execute_reply":"2025-03-22T13:47:34.216834Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/22146 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ede994429474e8c866a31576fc6b4a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2461 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e34b5c4ad7644d039588f70682d7f566"}},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"data_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T13:47:34.218709Z","iopub.execute_input":"2025-03-22T13:47:34.218968Z","iopub.status.idle":"2025-03-22T13:47:34.222515Z","shell.execute_reply.started":"2025-03-22T13:47:34.218947Z","shell.execute_reply":"2025-03-22T13:47:34.221859Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"training_args = TrainingArguments(\n     output_dir = \"./results\",\n        lr_scheduler_type = \"cosine\",\n        learning_rate = 2e-5,\n        weight_decay = 0.05,\n        warmup_ratio = 0.03,\n        per_device_train_batch_size = 4,\n        per_device_eval_batch_size = 4,\n        gradient_accumulation_steps = 16, \n        num_train_epochs = 2,\n        eval_strategy = \"steps\",\n        eval_steps = 100,\n        logging_steps = 100,\n        optim = \"adamw_bnb_8bit\",\n        report_to = \"none\",\n        fp16 = True,\n        logging_dir=\"./logs\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_train_dataset,\n    eval_dataset=tokenized_validation_dataset,\n    data_collator = data_collator,\n)\ntrain_result = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T13:49:22.976069Z","iopub.execute_input":"2025-03-22T13:49:22.976381Z","iopub.status.idle":"2025-03-22T22:18:23.558923Z","shell.execute_reply.started":"2025-03-22T13:49:22.976359Z","shell.execute_reply":"2025-03-22T22:18:23.557980Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='692' max='692' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [692/692 8:28:23, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>284.873200</td>\n      <td>15.438684</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>161.246700</td>\n      <td>2.383721</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>9.900100</td>\n      <td>0.346761</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>5.387200</td>\n      <td>0.318510</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>5.179600</td>\n      <td>0.307745</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>5.024400</td>\n      <td>0.303802</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"!zip -r checkpoint_v2_692.zip /kaggle/working/results/checkpoint-692","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T22:32:05.715386Z","iopub.execute_input":"2025-03-22T22:32:05.715701Z","iopub.status.idle":"2025-03-22T22:32:06.166269Z","shell.execute_reply.started":"2025-03-22T22:32:05.715674Z","shell.execute_reply":"2025-03-22T22:32:06.165119Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/results/checkpoint-692/ (stored 0%)\n  adding: kaggle/working/results/checkpoint-692/README.md (deflated 66%)\n  adding: kaggle/working/results/checkpoint-692/adapter_model.safetensors (deflated 8%)\n  adding: kaggle/working/results/checkpoint-692/optimizer.pt (deflated 11%)\n  adding: kaggle/working/results/checkpoint-692/adapter_config.json (deflated 53%)\n  adding: kaggle/working/results/checkpoint-692/training_args.bin (deflated 51%)\n  adding: kaggle/working/results/checkpoint-692/scheduler.pt (deflated 56%)\n  adding: kaggle/working/results/checkpoint-692/rng_state.pth (deflated 25%)\n  adding: kaggle/working/results/checkpoint-692/trainer_state.json (deflated 71%)\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"eval_results = trainer.evaluate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T22:34:00.410214Z","iopub.execute_input":"2025-03-22T22:34:00.410570Z","iopub.status.idle":"2025-03-22T22:45:17.589003Z","shell.execute_reply.started":"2025-03-22T22:34:00.410539Z","shell.execute_reply":"2025-03-22T22:45:17.588293Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='616' max='616' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [616/616 11:16]\n    </div>\n    "},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"eval_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T22:47:25.740546Z","iopub.execute_input":"2025-03-22T22:47:25.740885Z","iopub.status.idle":"2025-03-22T22:47:25.746116Z","shell.execute_reply.started":"2025-03-22T22:47:25.740859Z","shell.execute_reply":"2025-03-22T22:47:25.745259Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 0.30329880118370056,\n 'eval_runtime': 677.1692,\n 'eval_samples_per_second': 3.634,\n 'eval_steps_per_second': 0.91,\n 'epoch': 1.9969297453494672}"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"import math\nperplexity = math.exp(eval_results['eval_loss'])\nperplexity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T00:00:51.994344Z","iopub.execute_input":"2025-03-23T00:00:51.994650Z","iopub.status.idle":"2025-03-23T00:00:51.999690Z","shell.execute_reply.started":"2025-03-23T00:00:51.994627Z","shell.execute_reply":"2025-03-23T00:00:51.998952Z"}},"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"1.3543190761331088"},"metadata":{}}],"execution_count":66},{"cell_type":"code","source":"from peft import PeftModel\n\nmodel_path = \"/kaggle/working/results/checkpoint-692\"\nfine_tuned_model = PeftModel.from_pretrained(base_model, model_path, torch_dtype=\"auto\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T00:02:02.380188Z","iopub.execute_input":"2025-03-23T00:02:02.380492Z","iopub.status.idle":"2025-03-23T00:02:02.487129Z","shell.execute_reply.started":"2025-03-23T00:02:02.380470Z","shell.execute_reply":"2025-03-23T00:02:02.486466Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"device = torch.device(\"cuda\")\ndef generate_response(prompt, model):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_new_tokens=100)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T00:02:06.339759Z","iopub.execute_input":"2025-03-23T00:02:06.340081Z","iopub.status.idle":"2025-03-23T00:02:06.344610Z","shell.execute_reply.started":"2025-03-23T00:02:06.340057Z","shell.execute_reply":"2025-03-23T00:02:06.343741Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"prompt = json.dumps([\n    {\"role\": \"system\", \"content\": \"You are a Indian legal expert providing concise summaries.\"}, \n    {\"role\": \"user\", \"content\": \"What is the penalty for using a forged document in India as genuine?\"},  \n    {\"role\": \"assistant\", \"content\": \"\"}  \n], ensure_ascii=False)\n\nresponse = generate_response(prompt, fine_tuned_model)\n\nprint(\"Response:\", response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T00:08:08.398309Z","iopub.execute_input":"2025-03-23T00:08:08.398602Z","iopub.status.idle":"2025-03-23T00:08:17.787835Z","shell.execute_reply.started":"2025-03-23T00:08:08.398580Z","shell.execute_reply":"2025-03-23T00:08:17.787097Z"}},"outputs":[{"name":"stdout","text":"Response: [{\"role\": \"system\", \"content\": \"You are a Indian legal expert providing concise summaries.\"}, {\"role\": \"user\", \"content\": \"What is the penalty for using a forged document in India as genuine?\"}, {\"role\": \"assistant\", \"content\": \"\"}] In India, if you use a forged document as genuine, the penalties can vary depending on the nature of the document and the consequences. However, generally, the punishment can be imprisonment up to 3 years or fine up to ₹50,000 (Indian Rupees Fifty Thousand) or both. For specific cases, the court may impose additional penalties based on the severity of the offense. Therefore, it is important to consult with a legal professional for accurate guidance on this matter.\"}\n","output_type":"stream"}],"execution_count":86}]}